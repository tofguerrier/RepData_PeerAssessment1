require(randomForest)
x1=rnorm(nobs)
x2=rnorm(nobs,mean = x1, sd = 1)
y=2*x1+rnorm(nobs,mean = 0, sd = .5)
df=data.frame(y,x1,x2,matrix(rep(rnorm(nobs),nvar),nrow = nobs))
# run the randomForest implementation
rf1 <- randomForest(y~., data=df, mtry=2, ntree=int(nvar*ratio),importance=TRUE)
result = importance(rf1,type=1)
return(result[,1])
}
#There is a random aspect so need to do that multiple times
for(i in seq(0,4, by=0.2) {
for(i in seq(0,4, by=0.2)) {
if(i != 0){
print(get_importance(nobs=1000,nvar=10, ratio=i))
}
}
?round
get_importance <- function(nobs=1000, nvar=50, ratio=10) {
require(randomForest)
x1=rnorm(nobs)
x2=rnorm(nobs,mean = x1, sd = 1)
y=2*x1+rnorm(nobs,mean = 0, sd = .5)
df=data.frame(y,x1,x2,matrix(rep(rnorm(nobs),nvar),nrow = nobs))
# run the randomForest implementation
rf1 <- randomForest(y~., data=df, mtry=2, ntree=floor(nvar*ratio),importance=TRUE)
result = importance(rf1,type=1)
return(result[,1])
}
#There is a random aspect so need to do that multiple times
for(i in seq(0,4, by=0.2)) {
if(i != 0){
print(get_importance(nobs=1000,nvar=10, ratio=i))
}
}
#There is a random aspect so need to do that multiple times
for(i in seq(0,4, by=0.01)) {
if(i != 0){
print("Ratio", i)
print(get_importance(nobs=1000,nvar=10, ratio=i))
}
}
print i
print(i)
print("ratio",i)
?print
print("Ratio"+i)
i
for(i in seq(0,4, by=0.1)) {
if(i != 0){
print("Ratio"+i)
print(get_importance(nobs=1000,nvar=10, ratio=i))
}
}
for(i in seq(0,4, by=0.1)) {
if(i != 0){
#print("Ratio"+i)
print(get_importance(nobs=1000,nvar=10, ratio=i))
}
}
max(pred1$preictal, pred2$preictal)
apply(cbind(pred1$preictal, pred2$preictal)), 1, max)
apply(cbind(pred1$preictal, pred2$preictal), 1, max)
apply(cbind(pred1$preictal, pred2$preictal), 2, max)
apply(cbind(pred1$preictal, pred2$preictal), 1, max)
apply(cbind(pred1$preictal, pred2$preictal), 1, which.max)
apply(cbind(pred1$preictal, pred2$preictal), 1, which.min)
pred1$preictal - pred2$preictal > 0
x <- 1:10
if(x > 5) {
x <- 0
}
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z <- 10
f(3)
fileurl = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv'
fileurl = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv'"
fileurl = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
acs <- read.csv(file = (url=fileurl), header = TRUE)
download.file(url = fileurl,destfile = "~/work/Coursera_homework/acs.csv", method = "curl")
acs <- read.csv(file = df, header = TRUE)
df <- "~/work/Coursera_homework/acs.csv"
acs <- read.csv(file = df, header = TRUE)
head(acs)
acs$VAL
acs$VAL == 24
sum(acs$VAL == 24)
sum(acs$VAL == 24, na.rm = TRUE)
acs$FES
summary(acs$FES)
fileurl = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
df <- "~/work/Coursera_homework/NGA.xlsx"
download.file(url = fileurl,destfile = df, method = "curl")
?"read.xls"
??"read.xls"
library(xlsx)
library("xlsx")
library(gdata)
xl <- read.xls(df)
dat <- xl[18:23,7-15]
dat
sum(dat$Zip*dat$Ext,na.rm=T)
head(xl)
dat
dat$Zip
install.packages("xlsx")
library(xlsx)
dat <- read.xlsx(fileurl,startRow = 18, endRow = 23,colIndex = 7:15)
dat <- read.xlsx(fileurl,sheetIndex = 1, startRow = 18, endRow = 23,colIndex = 7:15)
dat <- read.xlsx(df,sheetIndex = 1, startRow = 18, endRow = 23,colIndex = 7:15)
sum(dat$Zip*dat$Ext,na.rm=T)
library(xml)
library(XML)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
df <- "~/work/Coursera_homework/resto.xml"
download.file(url = fileurl,destfile = df, method = "curl")
?XML
??XML
doc <- xmlParse(df)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
df <- "~/work/Coursera_homework/resto.xml"
download.file(url = fileurl,destfile = df, method = "curl")
doc <- xmlParse(df)
doc <- xmlParseDoc(df)
doc <- xmlParse(https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml)
doc <- xmlParse(url(https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml))
doc <- xmlParse(url("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"))
doc <- xmlParse("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml")
doc <- xmlParseDoc("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml")
doc <- xmlParseDoc("~/work/Coursera_homework/getdata%2Fdata%2Frestaurants.xml")
docName
class(doc)
?xmlSApply
xmlSApply(xmlRoot(doc), xmlName)
xmlSApply(xmlRoot(doc), xmlAttr)
xmlSApply(xmlRoot(doc), xmlAttrs)
doc[[1]]
xmlRoot(doc)[[1]]
xmlRoot(doc)[[1]][[1]]
xmlRoot(doc)[[1]][[1]][[zipcode]]
xmlSApply(xmlRoot(doc),xmlValue)
class(xmlSApply(xmlRoot(doc),xmlValue))
xmlSApply(xmlRoot(doc),"//zipcode",xmlValue)
xmlRoot(doc)[[1]][[1]]
xmlSApply(xmlRoot(doc),"zipcode",xmlValue)
xmlSApply(xmlRoot(doc),"//zipcode",xmlValue)
xmlSApply(xmlRoot(doc),"//name",xmlValue)
xmlRoot(doc)[[1]][[1]]
xmlRoot(doc)[[1]
]
xmlRoot(doc)[[1]]
xmlSApply(xmlRoot(doc)[[1]],"//name",xmlValue)
xmlSApply(xmlRoot(doc)[[1]],"//zipcode",xmlValue)
xmlRoot(doc)[[1]][[1]]
xmlRoot(doc)[[1]][[1]][[1]]
xmlSApply(xmlRoot(doc)[[1]],"//li[@class='zipcode']",xmlValue)
xmlSApply(xmlRoot(doc),"//li[@class='zipcode']",xmlValue)
xmlSApply(xmlRoot(doc),"//[@class='zipcode']",xmlValue)
xmlSApply(xmlRoot(doc),"//'zipcode']",xmlValue)
xmlSApply(xmlRoot(doc),"//'zipcode'",xmlValue)
xmlSApply(xmlRoot(doc)[[1]][[1]][[1]],"//'zipcode'",xmlValue)
xmlSApply(xmlRoot(doc)[[1]][[1]][[1]][[1]],"//'zipcode'",xmlValue)
xmlSApply(xmlRoot(doc)[[1]][[1]][[1]][[1]],"//zipcode",xmlValue)
url = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
?read.csv
download.file(url, destfile = "~/work/Coursera_homework/pid.csv", method = "curl")
dt <- read.csv("~/work/Coursera_homework/pid.csv")
?fread
??fread
install_from_swirl("Getting and Cleaning Data")
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl()
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
cran <- tbl_df(mydf)
rm("mydf")
?tbl_df
cran
?manip
select(cran, ip_id, package, country)
5:20
select(cran,r_arch:country)
select(cran,country:r_arch)
cran
select(cran, -time)
-5:20
-(5:20)
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran, r_version == "3.1.1", country == "US")
?Comparison
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US" | country == "IN")
filter(cran, size > 100500, r_os == "linux-gnu")
is.na(c(3, 5, NA, 10))
!is.na(c(3, 5, NA, 10))
filter(cran, r_version == !is.na())
filter(cran, !is.na(r_version))
?select
cran2 <- select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <-  select(cran, ip_id, package,size)
cran3
mutate(cran3, size_mb = size / 2^20)
mutate(cran3, size_gb = size_mb / 2^10)
mutate(cran3, size_gb = size_mb / 2^10 )
cran3
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, correct_size = size - 1000)
mutate(cran3, correct_size = size + 1000)
summarize(cran,avg_bytes = mean(size))
install.packages("UsingR")
library(UsingR)
data(galton)
par(mfrow=c(2,1)
)
hist(galton$child, break=00)
hist(galton$child, break=100)
hist(galton$child, breaks=100)
hist(galton$parent, breaks=100)
plot(child ~ parent, data = galton)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
mean(w*x)
mean(x)
sqrt(mean(x))
sqrt(mean(w*x))
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y ~ x)
summary(lm(y ~ x,))
?lm
plot(x,y)
plot(y,x)
summary(lm(y ~ 0+x,))
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
w*x
sum(w*x)/sum(w)
data(mtcars)
lm(mpg ~ weight, data = mtcars)
names(mtcars)
?mtcars
lm(mpg ~ wt, data = mtcars)
1.5*0.4
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
scale(x,center = TRUE, scale = TRUE)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y ~ x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
sum(w*x)/sum(w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y ~ 0 + x)
lm(mpg ~ wt, data = mtcars)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
scale(x, center = TRUE, scale=TRUE)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y ~x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
?qunif
qunif(0.75)
quantile(x, c(.25, .50,  .75, .90, .99))
quantile(runif(100000), c(.25, .50,  .75, .90, .99)
)
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
mean(x)
temp[1,]
temp[1,]*temp[2,]
sum(temp[1,]*temp[2,])
sum(temp[1,]*temp[2,])/4
sum(temp[1,]*temp[2,])
urldata = "https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2Fhousehold_power_consumption.zip"
temp <- tempfile()
download.file(urldata,temp,method = "curl")
data <- read.table(unz(temp, "a1.dat"))
?unz
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
p
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
library(ggplot2)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
qplot(Wind, Ozone, data = airquality)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, geom = "smooth")
library(ggplot2)
g <- ggplot(movies, aes(votes, rating))
print(g)
g
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies) + stats_smooth("loess")
qplot(votes, rating, data = movies, panel = panel.loess)
qplot(votes, rating, data = movies, smooth = "loess")
qplot(votes, rating, data = movies) + geom_smooth()
rnorm(n=100000, mean = 0, sd = 1)
var(sample(rnorm(n=100000, mean = 0, sd = 1),size = 10))
var(sample(rnorm(n=100000, mean = 0, sd = 1),size = 10000))
var(sample(rnorm(n=100000, mean = 0, sd = 1),size = 1000))
var(sample(rnorm(n=100000, mean = 0, sd = 1),size = 1000))
pnorm(70, mean = 80, sd = 10)
library(XML)
url <- "http://www.gdacs.org/Cyclones/report.aspx?eventid=41058&episodeid=28&eventtype=TC"
dat <- readHTMLTable(readLines(url), which=5)
dat$latlon <- dat[,8]
levels(dat$latlon) <- sapply(
strsplit(levels(dat[,8]), ",\n        "),
function(x) paste(x[2], x[1], sep=":")
)
dat$Category <- factor(dat$Category, levels=levels(dat$Category)[c(6,7,1:5)],
ordered=TRUE)
dat$cat <- as.numeric(dat$Category)
dat$Gust_kmh <- dat[,6]
levels(dat$Gust_kmh) <- sapply(strsplit(levels(dat[,6]), "km"),
function(x) gsub(" ", "",x[1]))
dat$Gust_kmh <- as.numeric(as.character(dat$Gust_kmh))
library(googleVis)
M <- gvisGeoChart(dat, "latlon", sizevar="cat",
colorvar="Gust_kmh",
options=list(region='035',
backgroundColor="lightblue",
datalessRegionColor="grey"))
plot(M)
## compile a list of available packages on CRAN
aps   <- as.data.frame(available.packages())
## get the list of Depends and clean up a bit
deps  <- gdata::trim(unlist(strsplit(as.character(aps$Depends), ',')))
deps  <- gsub('[ \\(].*|\\n', '', deps)
## freq table
depst <- table(deps)
## plot it!
library(wordcloud)
wordcloud(names(depst), log(as.numeric(depst)), colors = c('green', 'red'))
library(AppliedPredictiveModeling)
install.packages(‘AppliedPredictiveModeling’)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
summary(concrete)
plot(CompressiveStrength)
plot(CompressiveStrength, data= concrete)
plot(concrete$CompressiveStrength, data= concrete)
plot(concrete$CompressiveStrength, col = concrete$Age)
plot(concrete$CompressiveStrength, col = concrete$FlyAsh)
plot(concrete$CompressiveStrength, col = concrete$FlyAsh + 10)
names(concrete)
plot(concrete$CompressiveStrength, col = concrete$Cement)
plot(concrete$CompressiveStrength, col = concrete$Water)
plot(concrete$CompressiveStrength, col = concrete$FineAggregate)
plot(traing$CompressiveStrength, col = training$FineAggregate)
plot(training$CompressiveStrength, col = training$FineAggregate)
plot(training$CompressiveStrength)
plot(training$CompressiveStrength, col=training$Age)
plot(training$CompressiveStrength, col=training$FlyAsh)
plot(training$CompressiveStrength, col=training$FlyAsh+1)
plot(training$CompressiveStrength, col=training$Cement)
plot(training$CompressiveStrength, col=training$Cement+1)
plot(training$CompressiveStrength, col=training$BlastFurnaceSlag+1)
plot(training$CompressiveStrength, col=training$Water+1)
plot(training$CompressiveStrength, col=training$Superplasticizer+1)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training$Superplasticizer)
hist(training$Superplasticizer,breaks = 20)
hist(log(training$Superplasticizer),breaks = 20)
hist(log(training$Superplasticizer + 1),breaks = 20)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names(training)
?preProcess
preProcess(adData[,"IL*"])
adData[,"IL*"]
adData[,"IL"]
grep("^IL", names(adData))
names[grep("^IL", names(adData))]
names(adData)[grep("^IL", names(adData))]
preProcess(adData[,names(adData)[grep("^IL", names(adData))]])
preProcess(adData[,names(adData)[grep("^IL", names(adData))]],method="pca")
preProcess(adData[,names(adData)[grep("^IL", names(adData))]],method="pca",thresh = 0.9)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
training$diagnosis
Ttraining = training[,c("diagnosis", names(adData)[grep("^IL", names(adData))])]
preProc  <- preProcess(adData[,c("diagnosis", names(adData)[grep("^IL", names(adData))])])
preProc  <- preProcess(adData[,c("diagnosis", names(adData)[grep("^IL", names(adData))])])
preProc  <- preProcess(
adData[,c("diagnosis", names(adData)[grep("^IL", names(adData))])])
nadDtata = adData[,c("diagnosis", names(adData)[grep("^IL", names(adData))])]
glm(nadDtata$diagnosis ~ nadDtata[,names(adData)[grep("^IL", names(adData))]])
glm(nadDtata$diagnosis ~ nadDtata[,names(nadDtata)[grep("^IL", names(nadDtata))])
glm(nadDtata$diagnosis ~ nadDtata[,names(nadDtata)[grep("^IL", names(nadDtata))]])
glm(y = nadDtata$diagnosis, x= nadDtata[,names(nadDtata)[grep("^IL", names(nadDtata))]],)
glm(y ~ x, y = nadDtata$diagnosis, x= nadDtata[,names(nadDtata)[grep("^IL", names(nadDtata))]],)
plot(training$CompressiveStrength, col=training$Superplasticizer+1)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
summary(concrete)
plot(training$CompressiveStrength, col=training$Superplasticizer+1)
names(concrete)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(log(training$Superplasticizer + 1),breaks = 20)
hist(training$Superplasticizer,breaks = 200)
log(0)
preProcess(adData[,names(adData)[grep("^IL", names(adData))]],method="pca",thresh = 0.9)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
preProcess(adData[,names(adData)[grep("^IL", names(adData))]],method="pca",thresh = 0.8)
setwd("~/work/Coursera_homework/Reproducible Research/P1")
setwd("~/work/Coursera_homework/Reproducible Research/P1/RepData_PeerAssessment1")
getwd()
activity = read.csv(file = "./activity.csv",header=TRUE)
library(dplyr)
library(ggplot2)
stepsinterval <- colSums(table(activity$steps, activity$interval))
plot(stepsinterval, type = 'l',
main = "Time series, steps activity, over few days.", ylab = "Steps", xlab="Time")
stepsinterval
stepsinterval <- table(activity$steps, activity$interval))
stepsinterval <- table(activity$steps, activity$interval)
stepsinterval
stepsinterval <- mean(table(activity$steps, activity$interval))
stepsinterval
stepsinterval <- apply(table(activity$steps, activity$interval),2,mean)
stepsinterval
plot(stepsinterval, type = 'l',
main = "Time series, steps activity, over few days.", ylab = "Steps", xlab="Time")
activity = read.csv(file = "./activity.csv",header=TRUE)
activity$day = as.Date(activity$date,format = "%Y-%m-%d")
activity$hour = floor(activity$interval/100)
activity$min = 100 *(activity$interval /100 - floor(activity$interval/100))
activity$time = as.POSIXct(x = paste(activity$date,activity$hour,activity$min),format = "%Y-%m-%d %H %M")
stepsDay <- colSums(table(activity$steps, activity$date))
stepsinterval <- apply(table(activity$steps, activity$interval),2,mean)
stepsinterval
stepsinterval <- apply(table(activity$steps, activity$interval)*activity$steps,2,mean)
